#+TITLE: High Performance Computing
#+AUTHOR: Emil VATAI
* Short Intro
** Emil VATAI
   - *Riken-CCS*, High performance artificial intelligence systems (JP, Tokyo)
     - Fugaku, 1st on top500.org
   - *The University of Tokyo*, IST, Suda-lab
     - Numerical algorithms, HPC + ML
   - *Eötvös Loránd University*, Faculty of Informatics, Dep. of Computer algebra
     - Teaching discrete mathematics and bunch of other stuff
   - <3 math+CS <3, weeb(?), judo, *emacs* (vim keys), btw I use *Arch*
     - *mml-book.com* and *RL* book sessions @ MLT
   - Jack of all trades (master of none)
** About this talk
   - Sanyam's and Mani's *High performance python* session
     - *Disclaimer*: didn't read it (yet)!!!
     - I was being a smarty-pants on slack
   - Python & High performance?! debatable!
     - I like to debate! ^_^
     - Python is super slow, but super "nice" (again! debatable)
     - Hopefully you'll understand what I mean by the end of the talk
       ^_^
     - I think, *Python* is good intro to *computing*,
       - so *High Performance Python* is a good intro to *HPC*
** What will be a take-away
   - "Holistic" o_O (very high level) *overview*
     - (Almost) from the electric components
     - through the OS and system software
     - Up to the programming language / app level
   - Explanations of basic concepts (which were asked on slack)
   - Guidelines which might come in handy
   - Some fundamental trade-offs
   - *ULTIMATE GOAL* to have a nice discussion after
* Math
  - Emil, math? Again? (i.e. why math?)
    #+begin_quote
      42.

      -- Answer to the Ultimate Question of Life, the Universe, and
      Everything, given by in /The Hitchhiker's Guide to the Galaxy/
      by Douglas Adams, calculated by an enormous *supercomputer named
      Deep Thought* over a period of 7.5 million years
    #+end_quote
  - English is important but math is importanter!
** Karatsuba algorithm
*** Bignum [code]
     #+BEGIN_SRC python :results output
       reg = 0
       for bit_idx in range(200):
           bit = 2 ** bit_idx
           reg += bit
           # print(f"{reg: >32b}") ### do both!!!
       print(f"reg: {reg}")
       print(f"reg+1: {reg + 1}")
     #+END_SRC

     #+RESULTS:
     : reg: 1606938044258990275541962092341162602522202993782792835301375
     : reg+1: 1606938044258990275541962092341162602522202993782792835301376

     #+BEGIN_SRC C :results output
       const int sizeof_int_bits = sizeof(int) * 8;
       printf("sizeof int in bits: %d\n", sizeof_int_bits);
       int reg = 0;
       for (int i = 0; i < sizeof_int_bits; i++) {
         int bit = 1 << i; // 2 ** i
         reg += bit;
         printf("%8x\n", reg);
       }
       printf("result: %u\n", reg);
       printf("result + 1: %u\n", reg + 1);
     #+END_SRC

     #+RESULTS:
     #+begin_example
     sizeof int in bits: 32
            1
            3
            7
            f
           1f
           3f
           7f
           ff
          1ff
          3ff
          7ff
          fff
         1fff
         3fff
         7fff
         ffff
        1ffff
        3ffff
        7ffff
        fffff
       1fffff
       3fffff
       7fffff
       ffffff
      1ffffff
      3ffffff
      7ffffff
      fffffff
     1fffffff
     3fffffff
     7fffffff
     ffffffff
     result: 4294967295
     result + 1: 0
     #+end_example

*** Bignum [math]
     - $x = x_1 B + x_0$ e.g. $x=23, B=10$ then $23 = 2 \times B + 3$
     - $x = x_n B^n + \cdots x_2 B^2 + x_1 B + B$
     - e.g. $x = 234 = 2 \cdot 10^2 + 3 \cdot 10 + 4$
     - We want to multiply $x=x_1 B + x_0$ and $y = y_1 B + y_0$
     - Result has double number of digits
       - $x \cdot y = z = z_3 B^3 + z_2 B^2 + z_1 B + z_0$
       - $z = (x^1 B + x_0) ( y^1 B + y_0)$
       - $z = x_1 y_1 B^2 + (x_0 y_1 + x_1 y_0) B + x_0 y_0$
       - ($z_3$ is the upper half of $x_1 y_1$)
*** Karatsuba
    - Traditional: $z_0 = x_0 y_0$, $z_1 = x_0 y_1 + x_1 y_0$, $z_2 = x_1 y_1$
    - $z_1 = x_0 y_1 + x_1 y_0$ (2mul, 1add)
    - $z_1 = x_0 y_1 + x_1 y_0 + (x_0 y_0 - x_0 y_0) + (x_1 y_1 - x_1 y_1)$
    - $z_1 = x_0 y_1 + x_0 y_0 + x_1 y_0 + x_1 y_1 - x_0 y_0 - x_1 y_1$
    - $z_1 = x_0 (y_1 + y_0) + x_1 (y_0 + y_1) - x_0 y_0 - x_1 y_1$
    - $z_1 = (x_0 + x_1) (y_0 + y_1) - x_0 y_0 - x_1 y_1$
    - $z_1 = (x_0 + x_1) (y_0 + y_1) - z_0 - z_2$
    - (2mul, 1add) -> (1mul, 4add)
    - [[./figs/recursion.jpg]]
** FFT
   - Draw!
   - $n^2$ vs $O(n \log n) = O(2 n \log n + n + n \log n)$
     - FFT: $n \log n$ for both inputs
     - $n$ element-wise multiplications
     - IFFT: $n \log n$
   - Schönhage–Strassen algorithm
     - $O(n \log n \log \log n)$
     - Fürer's algorithm, Galactic algorithms
** What do HPC people program?
   - Lots of things, mostly scientific software
   - BLAS: Basic Linear Algebra Subroutines (FORTRAN!)
   - LAPACK: Linear Algebra PACkage
   - LINPACK Benchmarks: http://top500.org
     #+BEGIN_SRC python :results output
       import numpy as np
       np.show_config()
     #+END_SRC

     #+RESULTS:
     #+begin_example
     blas_mkl_info:
       NOT AVAILABLE
     blis_info:
       NOT AVAILABLE
     openblas_info:
       NOT AVAILABLE
     atlas_3_10_blas_threads_info:
       NOT AVAILABLE
     atlas_3_10_blas_info:
       NOT AVAILABLE
     atlas_blas_threads_info:
       NOT AVAILABLE
     atlas_blas_info:
       NOT AVAILABLE
     accelerate_info:
       NOT AVAILABLE
     blas_info:
         libraries = ['cblas', 'blas', 'blas']
         library_dirs = ['/usr/lib64']
         include_dirs = ['/usr/local/include', '/usr/include']
         language = c
         define_macros = [('HAVE_CBLAS', None)]
     blas_opt_info:
         define_macros = [('NO_ATLAS_INFO', 1), ('HAVE_CBLAS', None)]
         libraries = ['cblas', 'blas', 'blas']
         library_dirs = ['/usr/lib64']
         include_dirs = ['/usr/local/include', '/usr/include']
         language = c
     lapack_mkl_info:
       NOT AVAILABLE
     openblas_lapack_info:
       NOT AVAILABLE
     openblas_clapack_info:
       NOT AVAILABLE
     flame_info:
       NOT AVAILABLE
     atlas_3_10_threads_info:
       NOT AVAILABLE
     atlas_3_10_info:
       NOT AVAILABLE
     atlas_threads_info:
       NOT AVAILABLE
     atlas_info:
       NOT AVAILABLE
     lapack_info:
         libraries = ['lapack', 'lapack']
         library_dirs = ['/usr/lib64']
         language = f77
     lapack_opt_info:
         libraries = ['lapack', 'lapack', 'cblas', 'blas', 'blas']
         library_dirs = ['/usr/lib64']
         language = c
         define_macros = [('NO_ATLAS_INFO', 1), ('HAVE_CBLAS', None)]
         include_dirs = ['/usr/local/include', '/usr/include']
     #+end_example

* Computer science
** Low level (close to the metal)
*** CPU Central processing unit
    - Clock
    - Registers and operations
    - PC register
    - http://mmix.cs.hm.edu/
      - Volume 1 Fascicle 1 of TAOCP
**** Assembly
     #+BEGIN_SRC asm :tangle asm_out.s
       # C-c C-v t
       # RDI, RSI, RDX, RCX, R8, R9
       # XMM0-XMM7, stack
       .extern printf
       .section .data
       hello:
           .asciz "RSI: %d\n"
       .section .text
       .globl main
       main:
               push %rbp

               mov $40, %rsi
               mov $2, %rdi
               add %rdi, %rsi

               mov $0, %rax
               lea hello(%rip), %rdi
               call printf

               pop  %rbp
               mov $0, %rax
               ret
     #+END_SRC
     #+BEGIN_SRC shell :results code
       make -B asm_out && ./asm_out
     #+END_SRC
     #+RESULTS:
     #+begin_src shell
     cc    asm_out.s   -o asm_out
     RSI: 42
     #+end_src

     #+RESULTS:

*** CPU tricks
    - objdump -d
    - Branch prediction
    - Pipeline (super-scalar) [draw]
    - CISC vs RISC
    - MMX, SSE, AVX
      #+BEGIN_SRC bash :results code
        cat /proc/cpuinfo | head -n30
      #+END_SRC

      #+RESULTS:
      #+begin_src bash
      processor	: 0
      vendor_id	: GenuineIntel
      cpu family	: 6
      model		: 142
      model name	: Intel(R) Core(TM) i7-8650U CPU @ 1.90GHz
      stepping	: 10
      microcode	: 0xd6
      cpu MHz		: 2300.014
      cache size	: 8192 KB
      physical id	: 0
      siblings	: 8
      core id		: 0
      cpu cores	: 4
      apicid		: 0
      initial apicid	: 0
      fpu		: yes
      fpu_exception	: yes
      cpuid level	: 22
      wp		: yes
      flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d
      vmx flags	: vnmi preemption_timer invvpid ept_x_only ept_ad ept_1gb flexpriority tsc_offset vtpr mtf vapic ept vpid unrestricted_guest ple shadow_vmcs pml ept_mode_based_exec
      bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit srbds
      bogomips	: 4201.88
      clflush size	: 64
      cache_alignment	: 64
      address sizes	: 39 bits physical, 48 bits virtual
      power management:

      processor	: 1
      vendor_id	: GenuineIntel
      #+end_src

**** AVX extensions (in asm)
     #+BEGIN_SRC asm :tangle avx_prn.s
       # C-c C-v t
       # RDI, RSI, RDX, RCX, R8, R9
       # XMM0-XMM7, stack
       .extern printf
       .section .data
       hello:  .asciz "xmm: %lf %lf %lf %lf (extra: %lf)\n"
       data:   .double 1.0, 2.0, 3.0, 4.0
       .section .text
       .globl main
       main:
               push %rbp

               vmovupd data(%rip), %ymm0
               vmulpd %ymm0, %ymm0, %ymm0
               vmovupd %ymm0, data(%rip)

               // vhaddpd %ymm0, %ymm0, %ymm1
               // vphaddd %ymm1, %ymm1, %ymm1
               // vextractf128 $0, %ymm1, %xmm4
               // vextractf128 $1, %ymm1, %xmm5
               // addsd %xmm5, %xmm4

               // Call printf
               lea hello(%rip), %rdi
               movq 8*0+data(%rip), %xmm0
               movq 8*1+data(%rip), %xmm1
               movq 8*2+data(%rip), %xmm2
               movq 8*3+data(%rip), %xmm3
               mov $5, %rax
               call printf

               pop  %rbp
               mov $0, %rax
               ret
     #+END_SRC

     #+BEGIN_SRC shell
       make -B avx_prn && ./avx_prn
     #+END_SRC

     #+RESULTS:
     : cc    avx_prn.s   -o avx_prn

*** RAM: Random Access Memory
    - Memory hierarchy
    - Virtual memory
    - Cache line, Pages, multicore
    - Stack (fast?) & Heap (slow?) [draw]
      #+BEGIN_SRC C
        int fibonacci(int n) {
          int fib1, fib2;
          if (n <= 1) return n;
          else {
            fib1 = fibonacci(n-1);
            fib2 = fibonacci(n-2);
            return fib1 + fib2;
          }
        }
      #+END_SRC
    - Memory alignment
** Mid level (close to the OS)
   - ABI, function calls, System V
   - ELF: Executable and Linkable Format
     - [[./figs/elf.png]]
   - DWARF: Debugging With Attributed Record Formats
     - debugging data format
   - libraries: static, dynamic
   - LD_LIBRARY_PATH=... (RPATH=?)
** High level (where compilers take you)
*** Programming languages
    - Interpreted vs compiled
    - Dynamic vs static, weakly vs strongly typed
    - Python, C++, RUST!!!
    - Vector bool
      #+BEGIN_SRC C++ :includes <vector>, <iostream>, <algorithm>
        std::vector<int> vec = {1,3,2};
        std::sort(begin(vec), end(vec));
        for (size_t i = 0; i < vec.size(); i++)
            std::cout << vec[i] << std::endl;
      #+END_SRC

      #+RESULTS:
      | 1 |
      | 2 |
      | 3 |

*** Parallel
**** OpenMP
      - TAOMPP
      - Shared memory (all threads see everything)
      - Easier to program (debatable: all threads see everything)
      - Limited to machine
      - Can be used for e.g. avx
***** An example
         #+BEGIN_SRC C++ :flags -fopenmp
           #include <iostream>
           #include <chrono>
           using namespace std::chrono;
           int main() {
             size_t n = 100000000;
             double *array = new double[n];
             auto start = high_resolution_clock::now();
             #pragma omp parallel for
             for (int i = 0; i < n; i++) {
               double k = i + 1;
               array[i] += 1 / (k*k);
             }
             auto stop = high_resolution_clock::now();
             double sum = 0.0;
             for (int i = 0; i < n; i++)
               sum += array[i];
             auto duration = duration_cast<microseconds>(stop - start);
             std::cout << "sum1/x^2 " << sum << std::endl;
             std::cout << "pi " << 6 * sum / 3.14 << std::endl;
             std::cout << "duration " << duration.count() << "ms" << std::endl;
           }
         #+END_SRC

         #+RESULTS:
         | sum1/x^2 |  1.64493 |
         | pi       |  3.14319 |
         | duration | 495476ms |

**** MPI: Message Parsing Interface
     - Distributed memory (all processes only see their own stuff)
     - Harder to program (cleaner)
     - Can run across multiple nodes
     - Different implementations:
       - OpenMPI (not OpenMP), MVPICH, Intel's
     - Horovod
     - ./mympi.c
**** GPU, CUDA, the future
     - Oxen vs chickens
     - Chicken vs...
       [[./figs/trex-vs-chicken.jpg]]
     - Chickens vs...
       [[./figs/trex-vs-chickens.jpg]]
     - Nvidia NCCL
       - Direct GPU-GPU over different hosts
       - [DRAW]
     - Tensor Cores
     - TPU's
* Some very deep wisdom (~_~)
  - Knuth and switching levels
  - THE BOOKS!!!
    - Pragmatic programmer & Clean code & Literate programming
    - TAOCP
    - TAOMPP
    - Dragon book
    - Pillar book
    - Agner Fog's website
  - Trade-offs
    - Static vs dynamic programming languages
    - Distributed vs shared memory
  - Denard scaling
  - Moore's law
  - Amdhal's law
  - Numerical stability
  - *Important*: Switching levels (of abstraction) -- Don Knuth (I
    think)
* Closing
  - Is Python *super slow*?
  - Should you *care*?
  - Which *programming language is the best*?
  - [[./figs/batman.jpg]]

